{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa5a6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('amazon_train.csv')\n",
    "\n",
    "# نمایش چند خط اول داده‌ها برای بررسی\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99793f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "# کدگذاری شناسه‌های کاربر و محصول\n",
    "train_data['UserEnc'] = user_encoder.fit_transform(train_data['UserID'])\n",
    "train_data['ProductEnc'] = item_encoder.fit_transform(train_data['ProductID'])\n",
    "\n",
    "# نمایش چند خط اول داده‌ها برای بررسی\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a781fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# بررسی وجود NaN در ستون rating\n",
    "print(\"تعداد مقادیر NaN در ستون Rating:\", train_data['Rating'].isna().sum())\n",
    "\n",
    "# حذف ردیف‌های حاوی NaN در ستون rating\n",
    "train_data = train_data.dropna(subset=['Rating'])\n",
    "\n",
    "# بررسی دوباره برای اطمینان از حذف NaN\n",
    "print(\"تعداد مقادیر NaN پس از حذف:\", train_data['Rating'].isna().sum())\n",
    "\n",
    "# بررسی توازن مجموعه‌داده (توزیع امتیازات در ستون rating)\n",
    "print(\"\\nتوزیع اولیه امتیازات:\")\n",
    "print(train_data['Rating'].value_counts())\n",
    "\n",
    "# نمایش توزیع امتیازات با نمودار\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='Rating', data=train_data)\n",
    "plt.title('توزیع امتیازات در مجموعه‌داده')\n",
    "plt.xlabel('امتیاز')\n",
    "plt.ylabel('تعداد')\n",
    "plt.show()\n",
    "\n",
    "# نمونه‌افزایی تصادفی برای متوازن‌تر کردن مجموعه‌داده\n",
    "# تعریف استراتژی نمونه‌افزایی (به‌صورت دستی تنظیم می‌کنیم تا تفاوت چشم‌گیر نباشد)\n",
    "sampling_strategy = {1: int(train_data['Rating'].value_counts().max() * 0.5),  # برای دسته‌های کوچک‌تر\n",
    "                    2: int(train_data['Rating'].value_counts().max() * 0.6),\n",
    "                    3: int(train_data['Rating'].value_counts().max() * 0.7),\n",
    "                    4: int(train_data['Rating'].value_counts().max() * 0.8),\n",
    "                    5: train_data['Rating'].value_counts().max()}  # دسته اکثریت بدون تغییر\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X = train_data[['UserEnc', 'ProductEnc']]  # ویژگی‌های ورودی\n",
    "y = train_data['Rating']  # برچسب‌ها\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# ایجاد DataFrame جدید با داده‌های نمونه‌افزایی‌شده\n",
    "train_data_balanced = pd.DataFrame({\n",
    "    'UserID': user_encoder.inverse_transform(X_resampled['UserEnc']),\n",
    "    'ProductID': item_encoder.inverse_transform(X_resampled['ProductEnc']),\n",
    "    'Rating': y_resampled,\n",
    "    'UserEnc': X_resampled['UserEnc'],\n",
    "    'ProductEnc': X_resampled['ProductEnc']\n",
    "})\n",
    "\n",
    "# بررسی توزیع جدید\n",
    "print(\"\\nتوزیع پس از نمونه‌افزایی:\")\n",
    "print(train_data_balanced['Rating'].value_counts())\n",
    "\n",
    "# نمایش توزیع جدید با نمودار\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='Rating', data=train_data_balanced)\n",
    "plt.title('توزیع امتیازات پس از نمونه‌افزایی')\n",
    "plt.xlabel('امتیاز')\n",
    "plt.ylabel('تعداد')\n",
    "plt.show()\n",
    "\n",
    "# نرمال‌سازی ستون Rating با MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_data_balanced['Rating_normalized'] = scaler.fit_transform(train_data_balanced[['Rating']])\n",
    "\n",
    "# نمایش چند خط اول داده‌ها برای بررسی\n",
    "print(\"\\nداده‌ها پس از نرمال‌سازی:\")\n",
    "print(train_data_balanced.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7759e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Add, Dense\n",
    "\n",
    "# تعداد کاربران و محصولات منحصربه‌فرد\n",
    "n_users = train_data_balanced['UserEnc'].nunique()\n",
    "n_items = train_data_balanced['ProductEnc'].nunique()\n",
    "\n",
    "# اندازه‌ی بردار تعبیه\n",
    "embedding_size = 50\n",
    "\n",
    "# تعریف ورودی‌ها\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "# لایه‌های تعبیه برای کاربر و کالا\n",
    "user_embedding = Embedding(input_dim=n_users, output_dim=embedding_size,\n",
    "                          name='user_embedding',\n",
    "                          embeddings_initializer='glorot_normal',\n",
    "                          embeddings_regularizer=keras.regularizers.l2(1e-6))(user_input)\n",
    "item_embedding = Embedding(input_dim=n_items, output_dim=embedding_size,\n",
    "                          name='item_embedding',\n",
    "                          embeddings_initializer='glorot_normal',\n",
    "                          embeddings_regularizer=keras.regularizers.l2(1e-6))(item_input)\n",
    "\n",
    "# لایه‌های تعبیه برای بایاس کاربر و کالا\n",
    "user_bias = Embedding(input_dim=n_users, output_dim=1,\n",
    "                     name='user_bias',\n",
    "                     embeddings_initializer='zeros')(user_input)\n",
    "item_bias = Embedding(input_dim=n_items, output_dim=1,\n",
    "                     name='item_bias',\n",
    "                     embeddings_initializer='zeros')(item_input)\n",
    "\n",
    "# مسطح کردن خروجی‌های تعبیه\n",
    "user_vec = Flatten()(user_embedding)\n",
    "item_vec = Flatten()(item_embedding)\n",
    "user_bias_vec = Flatten()(user_bias)\n",
    "item_bias_vec = Flatten()(item_bias)\n",
    "\n",
    "# ضرب نقطه‌ای بین بردارهای تعبیه کاربر و کالا\n",
    "dot_product = Dot(axes=1)([user_vec, item_vec])\n",
    "\n",
    "# جمع بایاس‌ها و ضرب نقطه‌ای\n",
    "output = Add()([dot_product, user_bias_vec, item_bias_vec])\n",
    "\n",
    "# اعمال تابع فعال‌ساز sigmoid\n",
    "output = Dense(1, activation='sigmoid', name='output')(output)\n",
    "\n",
    "# تعریف مدل\n",
    "model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "\n",
    "# تنظیم بهینه‌ساز با نرخ یادگیری اولیه\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# کامپایل مدل با تابع زیان binary_crossentropy\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['mae', 'mse'])\n",
    "\n",
    "# نمایش خلاصه مدل\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a060f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# آماده‌سازی داده‌ها برای آموزش\n",
    "X_train_user = train_data_balanced['UserEnc'].values\n",
    "X_train_item = train_data_balanced['ProductEnc'].values\n",
    "y_train = train_data_balanced['Rating_normalized'].values\n",
    "\n",
    "# تعریف callback‌ها برای بهبود آموزش\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n",
    "\n",
    "# آموزش مدل\n",
    "history = model.fit(\n",
    "    [X_train_user, X_train_item], y_train,\n",
    "    batch_size=64,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# نمایش نمودار خطا و معیارها\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# نمودار Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (Binary Crossentropy)')\n",
    "plt.legend()\n",
    "\n",
    "# نمودار MAE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aaf5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "\n",
    "# فرض می‌کنیم مدل قبلاً آموزش دیده و در 'best_model.keras' ذخیره شده است\n",
    "model = load_model('best_model.keras')\n",
    "\n",
    "# فرض می‌کنیم user_encoder و item_encoder از مرحله‌ی کدگذاری در دسترس هستند\n",
    "# همچنین scaler از مرحله‌ی نرمال‌سازی در دسترس است\n",
    "\n",
    "# خواندن داده‌های اعتبارسنجی\n",
    "valid_data = pd.read_csv('amazon_valid.csv')\n",
    "\n",
    "# فیلتر کردن شناسه‌های نادیده در داده‌های اعتبارسنجی\n",
    "valid_users = set(user_encoder.classes_)\n",
    "valid_items = set(item_encoder.classes_)\n",
    "valid_data = valid_data[valid_data['UserID'].isin(valid_users) & valid_data['ProductID'].isin(valid_items)]\n",
    "\n",
    "# بررسی تعداد ردیف‌های باقی‌مانده\n",
    "print(f\"تعداد ردیف‌های اعتبارسنجی پس از فیلتر کردن: {len(valid_data)}\")\n",
    "\n",
    "# کدگذاری شناسه‌های کاربر و کالا\n",
    "valid_data['UserEnc'] = user_encoder.transform(valid_data['UserID'])\n",
    "valid_data['ProductEnc'] = item_encoder.transform(valid_data['ProductID'])\n",
    "\n",
    "# نرمال‌سازی ستون rating\n",
    "valid_data['Rating_normalized'] = scaler.transform(valid_data[['Rating']])\n",
    "\n",
    "# آماده‌سازی داده‌های ورودی و هدف برای اعتبارسنجی\n",
    "X_valid_user = valid_data['UserEnc'].values\n",
    "X_valid_item = valid_data['ProductEnc'].values\n",
    "y_valid = valid_data['Rating_normalized'].values\n",
    "y_valid_true = valid_data['Rating'].values  # برچسب‌های اصلی (1 تا 5)\n",
    "\n",
    "# پیش‌بینی مدل روی داده‌های اعتبارسنجی\n",
    "y_valid_pred = model.predict([X_valid_user, X_valid_item], batch_size=64, verbose=1)\n",
    "\n",
    "# تبدیل خروجی‌های نرمال‌شده به برچسب‌های باینری (4 و 5 مثبت، بقیه منفی)\n",
    "# معادل‌سازی امتیازات نرمال‌شده به مقادیر اصلی: 4 و 5 در مقیاس اصلی به [0.75, 1] در مقیاس نرمال‌شده نگاشت می‌شوند\n",
    "threshold = 0.75  # آستانه برای امتیازات 4 و 5 (قابل تنظیم برای بهینه‌سازی Precision)\n",
    "y_valid_pred_binary = (y_valid_pred >= threshold).astype(int)  # پیش‌بینی‌های باینری\n",
    "y_valid_true_binary = (y_valid_true >= 4).astype(int)  # برچسب‌های واقعی باینری\n",
    "\n",
    "# محاسبه Precision\n",
    "precision = precision_score(y_valid_true_binary, y_valid_pred_binary)\n",
    "print(f\"Precision روی مجموعه‌ی اعتبارسنجی: {precision:.4f}\")\n",
    "\n",
    "# بررسی توزیع پیش‌بینی‌ها\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(y_valid_pred, bins=50, label='پیش‌بینی‌های نرمال‌شده')\n",
    "plt.axvline(x=threshold, color='r', linestyle='--', label=f'آستانه = {threshold}')\n",
    "plt.title('توزیع پیش‌بینی‌های مدل روی مجموعه‌ی اعتبارسنجی')\n",
    "plt.xlabel('امتیاز نرمال‌شده پیش‌بینی‌شده')\n",
    "plt.ylabel('تعداد')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# بهینه‌سازی آستانه برای بهبود Precision\n",
    "thresholds = np.arange(0.5, 0.9, 0.05)\n",
    "precisions = []\n",
    "for thresh in thresholds:\n",
    "    y_pred_binary = (y_valid_pred >= thresh).astype(int)\n",
    "    prec = precision_score(y_valid_true_binary, y_pred_binary)\n",
    "    precisions.append(prec)\n",
    "    print(f\"Threshold: {thresh:.2f}, Precision: {prec:.4f}\")\n",
    "\n",
    "# رسم نمودار Precision نسبت به آستانه\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, precisions, marker='o')\n",
    "plt.title('Precision در برابر آستانه‌های مختلف')\n",
    "plt.xlabel('آستانه')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# انتخاب بهترین آستانه\n",
    "best_threshold = thresholds[np.argmax(precisions)]\n",
    "print(f\"بهترین آستانه: {best_threshold:.2f}, Precision: {max(precisions):.4f}\")\n",
    "\n",
    "# آماده‌سازی پیش‌بینی‌ها برای مجموعه‌ی آزمون\n",
    "test_data = pd.read_csv('amazon_test.csv')\n",
    "\n",
    "test_data = test_data[test_data['UserID'].isin(valid_users) & test_data['ProductID'].isin(valid_items)]\n",
    "print(f\"تعداد ردیف‌های آزمون پس از فیلتر کردن: {len(test_data)}\")\n",
    "# کدگذاری شناسه‌های کاربر و کالا\n",
    "test_data['UserEnc'] = user_encoder.transform(test_data['UserID'])\n",
    "test_data['ProductEnc'] = item_encoder.transform(test_data['ProductID'])\n",
    "\n",
    "# آماده‌سازی داده‌های ورودی برای آزمون\n",
    "X_test_user = test_data['UserEnc'].values\n",
    "X_test_item = test_data['ProductEnc'].values\n",
    "\n",
    "# پیش‌بینی مدل روی داده‌های آزمون\n",
    "y_test_pred = model.predict([X_test_user, X_test_item], batch_size=64, verbose=1)\n",
    "\n",
    "# تبدیل پیش‌بینی‌ها به برچسب‌های باینری با بهترین آستانه\n",
    "y_test_pred_binary = (y_test_pred >= best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "703dcf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# تبدیل امتیازات نرمال‌شده (بازه‌ی [0, 1]) به مقیاس اصلی (1 تا 5)\n",
    "# فرض می‌کنیم MinMaxScaler روی مقادیر اصلی 1 تا 5 اعمال شده است\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "# گرد کردن امتیازات به نزدیک‌ترین عدد صحیح بین 1 تا 5\n",
    "y_test_pred_rounded = np.clip(np.round(y_test_pred), 1, 5).astype(int)\n",
    "\n",
    "# ایجاد DataFrame برای submission\n",
    "submission = pd.DataFrame({\n",
    "    'UserID': test_data['UserID'],\n",
    "    'ProductID': test_data['ProductID'],\n",
    "    'Rating': y_test_pred_rounded.flatten()\n",
    "})\n",
    "submission"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
